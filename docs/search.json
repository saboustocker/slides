[
  {
    "objectID": "slide_overview.html",
    "href": "slide_overview.html",
    "title": "Slides Listing",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nSubtitle\n\n\n\nDate\n\n\n\nproject\n\n\n\n\n\n\n\n\nOrigin Ambiguity\n\n\nWhat happens if people no longer know if something originates from AI or human authorship?\n\n\n29.10.2025\n\n\nAI assessment\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "IBT-HSG - Slides",
    "section": "",
    "text": "On this github page, I collect my slidedecks and project information collections as a knowledge hub for myself.\nWhile you are welcome to look at this page, please note that the materials uploaded here are not edited to provide stand-alone insights into projects without context, and if you stumbled on here on accident, they might be a bit confusing.\nAll materials displayed on this website are my own and do not contain private, sensitive or confidential information."
  },
  {
    "objectID": "index.html#slidedecks-and-project-collections",
    "href": "index.html#slidedecks-and-project-collections",
    "title": "IBT-HSG - Slides",
    "section": "",
    "text": "On this github page, I collect my slidedecks and project information collections as a knowledge hub for myself.\nWhile you are welcome to look at this page, please note that the materials uploaded here are not edited to provide stand-alone insights into projects without context, and if you stumbled on here on accident, they might be a bit confusing.\nAll materials displayed on this website are my own and do not contain private, sensitive or confidential information."
  },
  {
    "objectID": "slides/ambiguity.html",
    "href": "slides/ambiguity.html",
    "title": "Origin Ambiguity",
    "section": "",
    "text": "Differences in perception of trustworthiness, purchase intention, attribution & responsibility, and behavior\nAbility to differentiate between human origin and AI origin\nConsequences of ambiguity of origin\nLegal grounds for disclosure\n\n\n\n\n\n\npeople trust AI less than humans in various contexts:\n\nAI is perceived to lack human abilities (Kirk and Givi 2025; Lefkeli, Karataş, and Gürhan-Canli 2024)\n\nwhich leads to perceived human decision-makers in the background, which in turn leads to perceived reduction in privacy and increase in feeling exploitet (Lefkeli, Karataş, and Gürhan-Canli 2024)\n\nAI is perceived as less private (Lefkeli, Karataş, and Gürhan-Canli 2024)\ntrust is decreased more if AI use is disclosed (Schilke and Reimann 2025; Kirk and Givi 2025) …\n… and people trust AI less when disclosing things (Lefkeli, Karataş, and Gürhan-Canli 2024)\n\nsome of these effects (e.g. algorithm aversion (Bigman 2022)) have been discussed extensively already\nPfeuffer et al. (2025) show that disclosing the alteration of images decreases trust in content creators / brands\n\n\n[!NOTE] Notes This is basically “done” at least from the POV of getting any of this published. Trust as an attitude is a pretty basic and thus relatively uninteresting variable imo. It is just a positively valenced variable that can capture both cognitive and affective dimensions, thus it might work as mediator whenever there is either a positive or negative outcome.\n\n\n\n\n\nthe use of AI decreases the intention to purchase a product, the product loyalty and the willingness to recommend a product\n\nif communication is perceived to be written by AI (Kirk and Givi 2025)\nif generated imagery is generated by AI (Belanche et al. 2025)\n\nespecially for hedonic services (Belanche et al. 2025)\n\n… but if people trust AI, the effect is mitigated (Jung et al. 2025)\n… and perceived humanness of AI increases purchase intention (Jung et al. 2025)\n\n\n\n[!NOTE] Note My general suggestion would be to look at something that goes beyond purchase/acceptance. The dominant paradigm over the last years has been this aversion vs. acceptance debate. I think we have considerably more upside, if we focus on a more up- or downstream variable along consumers’ journeys with AI (which can be a variable with impact on business or consumers’ self concept etc.). That means, we think about something that is prior to acceptance or not directly relevant to it or we look at usage/interaction etc.\n\n\n\n\n\nbecause AI is perceived as lacking human abilities (Lefkeli, Karataş, and Gürhan-Canli 2024; Bigman 2022), responsibility is shifted to “humans in the background”\nThis leads to perceiving AI as less biased …(Feldkamp et al. 2024; Bigman 2022; Bonezzi and Ostinelli 2021; Bedemariam and Wessel 2023)\n… and to less moral outcry when AI makes “biased” decisions (Feldkamp et al. 2024; Bigman 2022; Bonezzi and Ostinelli 2021; Bedemariam and Wessel 2023)\n\n\n\n\n\nPeople display themselves as more analytical when they believe they are assessed by AI instead of humans (Goergen, De Bellis, and Klesse 2025)\n… (more phenomena in that direction to be researched)\n\n\n\n\n\n\n“almost 50% of respondents can distinguish between AI and human generated artwork.” (Vukojičić, Krstić, and Veinović 2023)\n“participants performed below chance levels in identifying AI-generated poems (46.6% accuracy)” (Porter and Machery 2024) “participants were more likely to judge AI-generated poems as human-authored than actual human-authored poems (Porter and Machery 2024)\nnon-artists have pronounced difficulty identifying AI generated images (Ha et al. 2024), but artists and professional artists had much less difficulty (Ha et al. 2024)\nHowever, You et al. (2024) did not find a difference between creators and non-creators in accuracy of identifying images\nAI generated image detectors (e.g. Hive) had no difficulty detecting AI generated images (Ha et al. 2024)\nWith training, participants were able to correctly identify AI generated text correctyl 65% of the time; without training 55% - i.e. slightly above chance (Milička et al. 2025)\n\n\n\n\n\nthere is very little research on how ambiguity of origin (and authenticity) affect consumer perception, however:\n\nWhittaker et al. (2025) show that perception of AI alteration (with deepfakes) decreases intention to purchase, and humanness of deepfake increases perceived authenticity\nSilver, Newman, and Small (2021) show that perceived inauthenticity increases outrage at brands. One core driver of perceived inauthenticity is deception, another one is adulteration (i.e. “enhancing”)\nthe uncanny valley effect is perceived as highly unpleasant (Seyama and Nagayama 2007)\n\n\n\n[!NOTE] Note This is interesting. What does feeling ambiguity related to content origin (or something else in AI realm) do with the consumer? Feeling unease, compensatory behavior (in other domain), outrage, what inferences do consumers draw under ambiguity?, how do consumers deal with ambiguity in general –&gt; we probably fall back on some heuristics, can we identify a new one here and any consequences?\n\n\n[!NOTE] Idea How many people are actually experiencing this kind of ambiguity? Under TikTok videos (define domain), we could probably find references to “Is this AI?” that would be perfect ambiguity markers. That is descriptive in a first step. But then what other variables could be interesting to look at here? What does this ambiguity do to the video’s performance in terms of engagment etc. (issue will be control group, so potentially just correlative). What are ambiguity markers that consumers mention, what makes them certain of the origin? Where is the marketing relevance? We either do a pure consumer story (it does something with the consumer, wellbeing, self-concept etc.) or we extrapolate to companies that use AI for ads or so: What does ambiguity do with brand attitudes etc. (there is a gap here, I suppose)\n\n\n\n\n\nthere are no universal provisions to mandate AI disclosure (Vischer, 2024)\nthe existing laws leave enough leeway for companies to utilize AI without disclosure, justifying the question as to how consumers react when origin is ambiguous\n\nGDPR / FADP as part of the duty to inform people when personal data is obtained to be used with any sort of AI system (GDPR Art. 13/14, FADP Art. 19ff) and for which purposes (GDPR Art. 5(1) and FADP Art. 6(2-3)), as well as in the context of consent (GDPR Art. 4(11) & FADP Art. 6(6))\nSwiss Labour Act (Ordinance 3, Art. 26) prohibits behavioral monitoring in the workplace unless required. In that case they must be disclosed (Ordinance 3, Art. 6)\nFederal Act on Cartels and other Restraints of Competition and the Federal Act on Unfair Competition(CH) provides some regulations on transparency, copyright of competitors, and targeting (CartA,UCA)\nThe United States have several state-wide bills which regulate some uses of AI (e.g. AI transparency Act in California or the Colorado AI Act).\ncopyright laws\nthe AI Act in the EU (see next page for excerpts)\nand more\n\n\n\n\nProviders shall ensure that AI systems intended to interact directly with natural persons are designed and developed in such a way that the natural persons concerned are informed that they are interacting with an AI system, unless this is obvious from the point of view of a natural person who is reasonably well-informed, observant and circumspect, taking into account the circumstances and the context of use. This obligation shall not apply to AI systems authorised by law to detect, prevent, investigate or prosecute criminal offences, subject to appropriate safeguards for the rights and freedoms of third parties, unless those systems are available for the public to report a criminal offence.\nProviders of AI systems, including general-purpose AI systems, generating synthetic audio, image, video or text content, shall ensure that the outputs of the AI system are marked in a machine-readable format and detectable as artificially generated or manipulated.\nDeployers of an AI system that generates or manipulates image, audio or video content constituting a deep fake, shall disclose that the content has been artificially generated or manipulated. This obligation shall not apply where the use is authorised by law to detect, prevent, investigate or prosecute criminal offence.\nWhere the content forms part of an evidently artistic, creative, satirical, fictional or analogous work or programme, the transparency obligations set out in this paragraph are limited to disclosure of the existence of such generated or manipulated content in an appropriate manner that does not hamper the display or enjoyment of the work.\nThe information referred to in paragraphs 1 to 4 shall be provided to the natural persons concerned in a clear and distinguishable manner at the latest at the time of the first interaction or exposure.\nAI Act, Art. 50\n\n\n\n\nIn general, AI generated materials cannot be copyrighted by companies\n\n\n\n\n\n\n\n\nPeople prefer human over AI\nOrganizations and Companies tend to prefer AI over human due to efficiency and cost*\nPeople are bad at identifying AI generated content\nThe law does not provide clear regulations on the (mandated) disclosure of AI, and research shows that AI disclosure decreases trust and purchase intention \\(\\rightarrow\\) motivation for companies to not disclose the use of AI\n\n*although it must be noted that many of these AI applications are still works in progress as well, and this sentiment could very well shift\n\n\n\n\nResearch on perception of AI- vs. human origin discloses AI vs. human origin \\(\\rightarrow\\) this is not the case in the real world\nDoes, and if yes, how does response differ in uncertain territory?\n\n\n\n\n\n\nthere is legitimate interest on the side of companies to not disclose AI (negative effects of disclosure: (Whittaker et al. 2025; Schilke and Reimann 2025; Lefkeli, Karataş, and Gürhan-Canli 2024))\nTo the best of my knowledge, there are not yet any studies analyzing the effect ambiguity of origin has on consumer perception of marketing and products\n(help here needed)"
  },
  {
    "objectID": "slides/ambiguity.html#differences-between-human-and-ai-origin",
    "href": "slides/ambiguity.html#differences-between-human-and-ai-origin",
    "title": "Origin Ambiguity",
    "section": "",
    "text": "people trust AI less than humans in various contexts:\n\nAI is perceived to lack human abilities (Kirk and Givi 2025; Lefkeli, Karataş, and Gürhan-Canli 2024)\n\nwhich leads to perceived human decision-makers in the background, which in turn leads to perceived reduction in privacy and increase in feeling exploitet (Lefkeli, Karataş, and Gürhan-Canli 2024)\n\nAI is perceived as less private (Lefkeli, Karataş, and Gürhan-Canli 2024)\ntrust is decreased more if AI use is disclosed (Schilke and Reimann 2025; Kirk and Givi 2025) …\n… and people trust AI less when disclosing things (Lefkeli, Karataş, and Gürhan-Canli 2024)\n\nsome of these effects (e.g. algorithm aversion (Bigman 2022)) have been discussed extensively already\nPfeuffer et al. (2025) show that disclosing the alteration of images decreases trust in content creators / brands\n\n\n[!NOTE] Notes This is basically “done” at least from the POV of getting any of this published. Trust as an attitude is a pretty basic and thus relatively uninteresting variable imo. It is just a positively valenced variable that can capture both cognitive and affective dimensions, thus it might work as mediator whenever there is either a positive or negative outcome.\n\n\n\n\n\nthe use of AI decreases the intention to purchase a product, the product loyalty and the willingness to recommend a product\n\nif communication is perceived to be written by AI (Kirk and Givi 2025)\nif generated imagery is generated by AI (Belanche et al. 2025)\n\nespecially for hedonic services (Belanche et al. 2025)\n\n… but if people trust AI, the effect is mitigated (Jung et al. 2025)\n… and perceived humanness of AI increases purchase intention (Jung et al. 2025)\n\n\n\n[!NOTE] Note My general suggestion would be to look at something that goes beyond purchase/acceptance. The dominant paradigm over the last years has been this aversion vs. acceptance debate. I think we have considerably more upside, if we focus on a more up- or downstream variable along consumers’ journeys with AI (which can be a variable with impact on business or consumers’ self concept etc.). That means, we think about something that is prior to acceptance or not directly relevant to it or we look at usage/interaction etc.\n\n\n\n\n\nbecause AI is perceived as lacking human abilities (Lefkeli, Karataş, and Gürhan-Canli 2024; Bigman 2022), responsibility is shifted to “humans in the background”\nThis leads to perceiving AI as less biased …(Feldkamp et al. 2024; Bigman 2022; Bonezzi and Ostinelli 2021; Bedemariam and Wessel 2023)\n… and to less moral outcry when AI makes “biased” decisions (Feldkamp et al. 2024; Bigman 2022; Bonezzi and Ostinelli 2021; Bedemariam and Wessel 2023)\n\n\n\n\n\nPeople display themselves as more analytical when they believe they are assessed by AI instead of humans (Goergen, De Bellis, and Klesse 2025)\n… (more phenomena in that direction to be researched)"
  },
  {
    "objectID": "slides/ambiguity.html#ability-to-differenciate-between-human-and-ai-origin",
    "href": "slides/ambiguity.html#ability-to-differenciate-between-human-and-ai-origin",
    "title": "Origin Ambiguity",
    "section": "",
    "text": "“almost 50% of respondents can distinguish between AI and human generated artwork.” (Vukojičić, Krstić, and Veinović 2023)\n“participants performed below chance levels in identifying AI-generated poems (46.6% accuracy)” (Porter and Machery 2024) “participants were more likely to judge AI-generated poems as human-authored than actual human-authored poems (Porter and Machery 2024)\nnon-artists have pronounced difficulty identifying AI generated images (Ha et al. 2024), but artists and professional artists had much less difficulty (Ha et al. 2024)\nHowever, You et al. (2024) did not find a difference between creators and non-creators in accuracy of identifying images\nAI generated image detectors (e.g. Hive) had no difficulty detecting AI generated images (Ha et al. 2024)\nWith training, participants were able to correctly identify AI generated text correctyl 65% of the time; without training 55% - i.e. slightly above chance (Milička et al. 2025)"
  },
  {
    "objectID": "slides/ambiguity.html#consequences-in-ambiguity-of-marketing-materials",
    "href": "slides/ambiguity.html#consequences-in-ambiguity-of-marketing-materials",
    "title": "Origin Ambiguity",
    "section": "",
    "text": "there is very little research on how ambiguity of origin (and authenticity) affect consumer perception, however:\n\nWhittaker et al. (2025) show that perception of AI alteration (with deepfakes) decreases intention to purchase, and humanness of deepfake increases perceived authenticity\nSilver, Newman, and Small (2021) show that perceived inauthenticity increases outrage at brands. One core driver of perceived inauthenticity is deception, another one is adulteration (i.e. “enhancing”)\nthe uncanny valley effect is perceived as highly unpleasant (Seyama and Nagayama 2007)\n\n\n\n[!NOTE] Note This is interesting. What does feeling ambiguity related to content origin (or something else in AI realm) do with the consumer? Feeling unease, compensatory behavior (in other domain), outrage, what inferences do consumers draw under ambiguity?, how do consumers deal with ambiguity in general –&gt; we probably fall back on some heuristics, can we identify a new one here and any consequences?\n\n\n[!NOTE] Idea How many people are actually experiencing this kind of ambiguity? Under TikTok videos (define domain), we could probably find references to “Is this AI?” that would be perfect ambiguity markers. That is descriptive in a first step. But then what other variables could be interesting to look at here? What does this ambiguity do to the video’s performance in terms of engagment etc. (issue will be control group, so potentially just correlative). What are ambiguity markers that consumers mention, what makes them certain of the origin? Where is the marketing relevance? We either do a pure consumer story (it does something with the consumer, wellbeing, self-concept etc.) or we extrapolate to companies that use AI for ads or so: What does ambiguity do with brand attitudes etc. (there is a gap here, I suppose)"
  },
  {
    "objectID": "slides/ambiguity.html#legal-grounds-on-disclosure-of-the-use-of-ai",
    "href": "slides/ambiguity.html#legal-grounds-on-disclosure-of-the-use-of-ai",
    "title": "Origin Ambiguity",
    "section": "",
    "text": "there are no universal provisions to mandate AI disclosure (Vischer, 2024)\nthe existing laws leave enough leeway for companies to utilize AI without disclosure, justifying the question as to how consumers react when origin is ambiguous\n\nGDPR / FADP as part of the duty to inform people when personal data is obtained to be used with any sort of AI system (GDPR Art. 13/14, FADP Art. 19ff) and for which purposes (GDPR Art. 5(1) and FADP Art. 6(2-3)), as well as in the context of consent (GDPR Art. 4(11) & FADP Art. 6(6))\nSwiss Labour Act (Ordinance 3, Art. 26) prohibits behavioral monitoring in the workplace unless required. In that case they must be disclosed (Ordinance 3, Art. 6)\nFederal Act on Cartels and other Restraints of Competition and the Federal Act on Unfair Competition(CH) provides some regulations on transparency, copyright of competitors, and targeting (CartA,UCA)\nThe United States have several state-wide bills which regulate some uses of AI (e.g. AI transparency Act in California or the Colorado AI Act).\ncopyright laws\nthe AI Act in the EU (see next page for excerpts)\nand more\n\n\n\n\nProviders shall ensure that AI systems intended to interact directly with natural persons are designed and developed in such a way that the natural persons concerned are informed that they are interacting with an AI system, unless this is obvious from the point of view of a natural person who is reasonably well-informed, observant and circumspect, taking into account the circumstances and the context of use. This obligation shall not apply to AI systems authorised by law to detect, prevent, investigate or prosecute criminal offences, subject to appropriate safeguards for the rights and freedoms of third parties, unless those systems are available for the public to report a criminal offence.\nProviders of AI systems, including general-purpose AI systems, generating synthetic audio, image, video or text content, shall ensure that the outputs of the AI system are marked in a machine-readable format and detectable as artificially generated or manipulated.\nDeployers of an AI system that generates or manipulates image, audio or video content constituting a deep fake, shall disclose that the content has been artificially generated or manipulated. This obligation shall not apply where the use is authorised by law to detect, prevent, investigate or prosecute criminal offence.\nWhere the content forms part of an evidently artistic, creative, satirical, fictional or analogous work or programme, the transparency obligations set out in this paragraph are limited to disclosure of the existence of such generated or manipulated content in an appropriate manner that does not hamper the display or enjoyment of the work.\nThe information referred to in paragraphs 1 to 4 shall be provided to the natural persons concerned in a clear and distinguishable manner at the latest at the time of the first interaction or exposure.\nAI Act, Art. 50\n\n\n\n\nIn general, AI generated materials cannot be copyrighted by companies"
  },
  {
    "objectID": "slides/ambiguity.html#identifying-the-gap",
    "href": "slides/ambiguity.html#identifying-the-gap",
    "title": "Origin Ambiguity",
    "section": "",
    "text": "People prefer human over AI\nOrganizations and Companies tend to prefer AI over human due to efficiency and cost*\nPeople are bad at identifying AI generated content\nThe law does not provide clear regulations on the (mandated) disclosure of AI, and research shows that AI disclosure decreases trust and purchase intention \\(\\rightarrow\\) motivation for companies to not disclose the use of AI\n\n*although it must be noted that many of these AI applications are still works in progress as well, and this sentiment could very well shift\n\n\n\n\nResearch on perception of AI- vs. human origin discloses AI vs. human origin \\(\\rightarrow\\) this is not the case in the real world\nDoes, and if yes, how does response differ in uncertain territory?\n\n\n\n\n\n\nthere is legitimate interest on the side of companies to not disclose AI (negative effects of disclosure: (Whittaker et al. 2025; Schilke and Reimann 2025; Lefkeli, Karataş, and Gürhan-Canli 2024))\nTo the best of my knowledge, there are not yet any studies analyzing the effect ambiguity of origin has on consumer perception of marketing and products\n(help here needed)"
  },
  {
    "objectID": "slides/ambiguity.html#next-steps",
    "href": "slides/ambiguity.html#next-steps",
    "title": "Origin Ambiguity",
    "section": "Next Steps",
    "text": "Next Steps\n\nIs this research area substantial enough to make a project out of it?\nWhat key considerations are we missing to make a project out of it?"
  }
]